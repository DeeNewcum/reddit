#!/usr/bin/perl

# TODO:   Create this script.
#
# At a high level, it should:
#
#       1. extract all links within a person's comments 
#                   (ie. links to external websites, made by the person, not generated by Reddit)
#
#       2. optionally, try to identify domains that might be associated with that person
#
#
#
# To accomplish the second item, use this:
# 
#       https://en.wikipedia.org/wiki/Public_Suffix_List
#               
# by way of one of these modules:
#
#       Mozilla::PublicSuffix
#       Domain::PublicSuffix
#       ParseUtil::Domain
#
#
#
# It could even have some specialized code to identify users at specific websites.  I'm particularly
# curious if it can identify parts of websites I control, for example:
#
#           https://github.com/DeeNewcum/

    use strict;
    use warnings;

    use JSON::XS;
    use Mozilla::PublicSuffix qw(public_suffix);
    #use Text::Markdown;

    use Data::Dumper;


#print Dumper [get_links_from_text(<<'EOF')];
#    https://paperlined.org/trades/automata/making_automata.md
#    [testing 1 2 3](https://paperlined.org/academics/united_states_outlier.html)
#EOF
#
#exit;


my $username = shift or die "specify a username\n";
$username =~ s/\.json$//s;


open my $fin, '<', "$username.json"     or die $!;
my $json = decode_json( do {local $/=undef; <$fin>} );

my @links;
foreach my $child (@{$json->{data}{children}}) {
    #======== post ========
    if ($child->{kind} eq 't3') {
        if ($child->{data}{domain} =~ /^self\./) {
            push @links, get_links_from_text($child->{data}{selftext});
        } elsif (exists $child->{data}{url} && $child->{data}{domain} !~ /^[iv]\.redd\.it$/) {
            push @links, $child->{data}{url};
        }

    #======== comment ========
    } elsif ($child->{kind} eq 't1') {
        push @links, get_links_from_text($child->{data}{body});
    }
}

my %domains;
foreach my $url (@links) {
    my $one_up = get_public_suffix_one_up($url);
    $domains{$one_up}++;
}

print Dumper \@links;

print "\n======== domains ========\n";
my @sorted = sort {$domains{$a} <=> $domains{$b}} keys %domains;
foreach my $domain (@sorted) {
    printf "%4d  https://%s\n", $domains{$domain}, $domain;
}


BEGIN {
    # see https://mathiasbynens.be/demo/url-regex
    my $url_re = '(?:(?:https?|ftp)://)(?:\S+(?::\S*)?@)?(?:(?!10(?:\.\d{1,3}){3})(?!127(?:\.\d{1,3}){3})(?!169\.254(?:\.\d{1,3}){2})(?!192\.168(?:\.\d{1,3}){2})(?!172\.(?:1[6-9]|2\d|3[0-1])(?:\.\d{1,3}){2})(?:[1-9]\d?|1\d\d|2[01]\d|22[0-3])(?:\.(?:1?\d{1,2}|2[0-4]\d|25[0-5])){2}(?:\.(?:[1-9]\d?|1\d\d|2[0-4]\d|25[0-4]))|(?:(?:[a-z\x{00a1}-\x{ffff}0-9]+-?)*[a-z\x{00a1}-\x{ffff}0-9]+)(?:\.(?:[a-z\x{00a1}-\x{ffff}0-9]+-?)*[a-z\x{00a1}-\x{ffff}0-9]+)*(?:\.(?:[a-z\x{00a1}-\x{ffff}]{2,})))(?::\d{2,5})?(?:/[^\s]*)?';

    # parse the markdown that's passed in, and return a list of links
    sub get_links_from_text {
        my $markdown = shift;

        # Actually, don't parse markdown. We want to find bare URLs too, so just look for anything that
        # looks like a URL.
        my @urls = ($markdown =~ m#$url_re#iog);

        # lop off trailing close-parens  (this should only apply when it's inside a markdown-link, but we're not that sophisticated)
        @urls = map {s/\).*//; $_} @urls;             
        
        # lop off markdown end-link syntax
        @urls = map {s/\]\(.*//; $_} @urls;             
    }
}


# Given a URL, return its domain name.
# Yes, there are much better ways t do this. I am eternally sorry.
# (no I'm not)
sub get_domain_name {
    my $url = shift;

    $url =~ s#^.*?//##;
    $url =~ s#/.*##;
    return $url;
}


sub get_public_suffix_one_up {
    my $url = shift;
    my $domain = get_domain_name($url);
    my $public_suffix = public_suffix($domain) || '';
    my @bits = split /\./, $domain;
    my $count = () = $public_suffix =~ /\./g;       # number of periods in $public_suffix
    my $one_up = $bits[-1 * ($count + 2)] . "." . $public_suffix;
    #print "$domain\t\t\t$one_up\t\t\t$count\n";
    return $one_up;
}
